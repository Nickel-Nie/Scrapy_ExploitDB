import scrapy
import logging
import datetime
import json
from urllib.parse import urlencode
from ..items import ExploitItem

today = datetime.datetime.now()
log_file_path = "./log/scrapy_{}_{}_{}.log".format(today.year, today.month, today.day)
logger = logging.getLogger(__name__)
logging.basicConfig(format='%(levelname)s'
                           '   [%(filename)s:%(lineno)d]   msg:  %(message)s'
                           ' - %(asctime)s',
                    datefmt='[%d/%b/%Y %H:%M:%S]',
                    filename=log_file_path,
                    level=logging.WARNING)

class SpiderSpider(scrapy.Spider):
    name = 'spider'
    recordsFiltered = 0  # 总共有多少条目，用于后面退出
    start = 0
    length = 15
    author = '3211'
    platform = 'windows'
    type = 'remote'
    allowed_domains = ['www.exploit-db.com']
    # start_url = 'https://www.exploit-db.com/exploits/'
    base_url = "https://www.exploit-db.com/"
    query_string_file = 'QueryStringParams.txt'

    def start_requests(self):
        query_string = self.get_query_string_params(start=self.start,
                                                    length=self.length,
                                                    author=self.author,
                                                    platform=self.platform,
                                                    type=self.type)
        request_url = self.base_url + '?' + query_string  # 需要发送请求的url
        yield scrapy.Request(request_url, callback=self.parse_url, dont_filter=True)  # 启动页面不能被过滤？

    def parse_url(self, response):
        try:
            data = json.loads(response.text)
            # logger.info(data)  # 得到json数据
        except Exception as e:
            logger.error('爬取网页错误：'+str(e))
        else:  # 如果没有异常发生
            self.recordsFiltered = data.get('recordsFiltered')
            if self.start >= self.recordsFiltered:
                return

            for exploit_data in data.get('data'):
                EDB_id = exploit_data.get('id')  # 这应该是唯一标识id
                url = self.base_url + 'exploits/' + str(EDB_id)
                yield scrapy.Request(url, callback=self.parse_item, meta={'id': EDB_id})

            # 继续爬下一页
            self.start += self.length
            query_string = self.get_query_string_params(start=self.start,
                                                        length=self.length,
                                                        author=self.author,
                                                        platform=self.platform,
                                                        type=self.type)
            request_url = self.base_url + '?' + query_string  # 需要发送请求的url
            yield scrapy.Request(request_url, callback=self.parse_url, dont_filter=True)

    def parse_item(self, response):
        exploit_item = ExploitItem()
        exploit_item['id'] = response.meta.get('id')
        exploit_item['filename'] = './exploits_code/exploit_{}.msf'.format(exploit_item['id'])
        exploit_item['name'] = response.xpath('/html/body/div[1]/div[2]/div[2]/div/div/div[1]/div/div[1]/h1/text()').extract_first().strip()
        exploit_item['author'] = response.xpath('/html/body/div[1]/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[2]/div/div[1]/div/div/div/div[1]/h6/a/text()').extract_first().strip()
        exploit_item['disclosure_date'] = response.xpath('/html/body/div[1]/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[3]/div/div[1]/div/div/div/div[2]/h6/text()').extract_first().strip()

        # 获得CVE编号，需要注意的是：可能没有CVE编号，可能只有一个，可能有多个：
        # 有一个和多个cve编号的先都作为一个来考虑
        cve_title = response.xpath("/html/body/div[1]/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[1]/div/div[1]/div/div/div/div[2]/h6")
        if cve_title.xpath('./text()').extract_first().strip() == 'N/A' or cve_title.xpath('./a').extract_first() == None:
            exploit_item['cve_id'] = 'N/A'
        else:
            exploit_item['cve_id'] = 'CVE-' + cve_title.xpath('./a[1]/text()').extract_first().strip()

        exploit_item['code'] = response.css('code::text').extract_first()
        # exploit_item['code'] = response.xpath('/html/body/div[1]/div[2]/div[2]/div/div/div[2]/div[1]/div/pre/code/text()').extract()  #这样拿不到代码？
        # logger.info(exploit_item['code'])
        exploit_item['platform'] = response.xpath('/html/body/div[1]/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[3]/div/div[1]/div/div/div/div[1]/h6/a/text()').extract_first().strip()
        exploit_item['type'] = response.xpath('/html/body/div[1]/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[2]/div/div[1]/div/div/div/div[2]/h6/a/text()').extract_first().strip()
        yield exploit_item

    def get_query_string_params(self, start=0, length=15, author='3211', platform="windows", type="remote"):
        """
        :param start: 起始的条目index。（0开始）
        :param length: 每一页条目数量
        :param author: 作者(3211为"metasploit")
        :param platform: 平台
        :param type: 类型
        :return: 编码后的请求字符串
        """
        with open(self.query_string_file, 'r') as f:
            query_string_list = f.readlines()

        query_string_dict = {}
        try:
            for param in query_string_list:
                k, v = param.strip().split(':')
                query_string_dict[k] = v

            query_string_dict['start'] = start
            query_string_dict['length'] = length
            query_string_dict['author'] = author
            query_string_dict['platform'] = platform
            query_string_dict['type'] = type
            query_string_dict['draw'] = 0

            return urlencode(query_string_dict)
        except Exception as e:
            logger.error('构造参数错误：'+str(e))
